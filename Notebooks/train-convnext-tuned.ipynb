{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from Models.EncoderModel import EncoderModelConvNeXt\n",
    "from Models.DecoderModel import DepthDecoderModel, PoseDecoderModel\n",
    "from Models.ESPCN import ESPCN\n",
    "from Losses.SSIM import SSIM\n",
    "from Models.BackprojectDepth import BackprojectDepth\n",
    "from Models.Project3D import Project3D\n",
    "from Dataset.KITTI import KITTI\n",
    "from utils import secondsToHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, LR=1e-4, batchSize=20, epochs=20, height=192, width=640, frameIdxs=[0, -1, 1],\n",
    "                 scales=[0, 1, 2, 3]):\n",
    "        self.LR = LR\n",
    "        self.batchSize = batchSize\n",
    "        self.epochs = epochs\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.frameIdxs = frameIdxs\n",
    "        self.numScales = len(scales)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        self.models = {}\n",
    "        self.totalTrainableParams = 0\n",
    "        self.trainableParameters = []\n",
    "        self.models[\"encoder\"] = EncoderModelConvNeXt()\n",
    "        self.models[\"encoder\"] = self.models[\"encoder\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"encoder\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"encoder\"].parameters() if p.requires_grad)\n",
    "        self.models[\"decoder\"] = DepthDecoderModel(self.models[\"encoder\"].numChannels)\n",
    "        self.models[\"decoder\"] = self.models[\"decoder\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"decoder\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"decoder\"].parameters() if p.requires_grad)\n",
    "        self.models[\"pose\"] = PoseDecoderModel(self.models[\"encoder\"].numChannels, 2, 1)\n",
    "        self.models[\"pose\"] = self.models[\"pose\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"pose\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"pose\"].parameters() if p.requires_grad)\n",
    "        self.ssim = SSIM()\n",
    "        self.ssim = self.ssim.to(self.device)\n",
    "        self.optimizer = optim.AdamW(self.trainableParameters, lr=self.LR, betas=(0.9, 0.999), weight_decay=0.05)\n",
    "        self.lrScheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, 20)\n",
    "        self.loadDataset()\n",
    "        self.depthMetricNames = [\"de/abs_rel\", \"de/sq_rel\", \"de/rms\", \"de/log_rms\", \"da/a1\", \"da/a2\", \"da/a3\"]\n",
    "        self.backprojectDepth = {}\n",
    "        self.project3d = {}\n",
    "        for scale in range(self.numScales):\n",
    "            h = self.height // (2**scale)\n",
    "            w = self.width // (2**scale)\n",
    "            self.backprojectDepth[scale] = BackprojectDepth(self.batchSize, h, w)\n",
    "            self.backprojectDepth[scale] = self.backprojectDepth[scale].to(self.device)\n",
    "            self.project3d[scale] = Project3D(self.batchSize, h, w)\n",
    "            self.project3d[scale] = self.project3d[scale].to(self.device)\n",
    "        self.writers = {}\n",
    "        for mode in [\"train\", \"val\"]:\n",
    "            self.writers[mode] = SummaryWriter(os.path.join(\"/scratch/mp6021/Monodepth2/logs-convnext-tuned-espcn\", mode))\n",
    "\n",
    "    def readlines(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "        return lines\n",
    "\n",
    "    def loadDataset(self):\n",
    "        self.dataset = KITTI\n",
    "        dataPath = os.path.join(\"/scratch/mp6021/Monodepth2\", \"data\", \"KITTI\")\n",
    "        filepath = os.path.join(dataPath, \"splits\", \"eigen_zhou\", \"{}_files.txt\")\n",
    "        trainFilenames = self.readlines(filepath.format(\"train\"))\n",
    "        valFilenames = self.readlines(filepath.format(\"val\"))\n",
    "        numTrain = len(trainFilenames)\n",
    "        self.numSteps = (numTrain//self.batchSize)*self.epochs\n",
    "        trainDataset = self.dataset(dataPath, trainFilenames, self.height, self.width,\n",
    "                                    self.frameIdxs, 4, True)\n",
    "        valDataset = self.dataset(dataPath, valFilenames, self.height, self.width, self.frameIdxs,\n",
    "                                  4, False)\n",
    "        self.trainLoader = DataLoader(trainDataset, self.batchSize, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "        self.valLoader = DataLoader(valDataset, self.batchSize, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "        self.valIterator = iter(self.valLoader)\n",
    "\n",
    "    def setTrain(self):\n",
    "        for model in self.models.values():\n",
    "            model.train()\n",
    "\n",
    "    def setEval(self):\n",
    "        for model in self.models.values():\n",
    "            model.eval()\n",
    "            \n",
    "    def log(self, mode, inputs, outputs, losses):\n",
    "        writer = self.writers[mode]\n",
    "        for lossname, value in losses.items():\n",
    "            writer.add_scalar(\"{}\".format(lossname), value, self.step)\n",
    "        for frameIdx in self.frameIdxs:\n",
    "            writer.add_image(\"color_{}\".format(frameIdx), inputs[(\"color\", frameIdx, 0)][0].data, self.step)\n",
    "            if frameIdx != 0:\n",
    "                writer.add_image(\"color_pred_{}\".format(frameIdx), outputs[(\"color\", frameIdx, 0)][0].data, self.step)\n",
    "            writer.add_image(\"disp\", self.normalizeImage(outputs[(\"disp\", 0)][0]), self.step)\n",
    "    \n",
    "    def logTime(self, batchIdx, duration, loss):\n",
    "        samplesPerSec = self.batchSize / duration\n",
    "        totalTime = time.time() - self.startTime\n",
    "        timeLeft = (self.numSteps / self.step - 1.0)*totalTime if self.step > 0 else 0\n",
    "        logString = \"Epoch : {:>3} | Batch : {:>7} | Step : {:>10} | examples/s: {:5.1f} | loss : {:.5f} | time elapsed: {} | time left: {}\"\n",
    "        print(logString.format(self.epoch, batchIdx, self.step, samplesPerSec, loss, secondsToHM(totalTime), secondsToHM(timeLeft)))\n",
    "\n",
    "    def saveModel(self):\n",
    "        outpath = os.path.join(\"/scratch/mp6021/Monodepth2\", \"models-convnext-tuned-espcn\", \"weights_{}\".format(self.epoch))\n",
    "        if not os.path.exists(outpath):\n",
    "            os.makedirs(outpath)\n",
    "        for name, model in self.models.items():\n",
    "            savePath = os.path.join(outpath, \"{}.pth\".format(name))\n",
    "            toSave = model.state_dict()\n",
    "            if name == \"encoder\":\n",
    "                toSave[\"height\"] = self.height\n",
    "                toSave[\"width\"] = self.width\n",
    "            torch.save(toSave, savePath)\n",
    "        savePath = os.path.join(outpath, \"adam.pth\")\n",
    "        torch.save(self.optimizer.state_dict(), savePath)\n",
    "        \n",
    "    def normalizeImage(self, image):\n",
    "        maxValue = float(image.max().cpu().data)\n",
    "        minValue = float(image.min().cpu().data)\n",
    "        diff = (maxValue - minValue) if maxValue != minValue else 1e5\n",
    "        return (image - minValue)/diff\n",
    "\n",
    "    def dispToDepth(self, disp, minDepth, maxDepth):\n",
    "        minDisp = 1 / maxDepth\n",
    "        maxDisp = 1 / minDepth\n",
    "        scaledDisp = minDisp + (maxDisp - minDisp)*disp\n",
    "        depth = 1 / scaledDisp\n",
    "        return scaledDisp, depth\n",
    "\n",
    "    def rotationFromAxisAngle(self, axisangle):\n",
    "        angle = torch.norm(axisangle, 2, 2, True)\n",
    "        axis = axisangle / (angle + 1e-7)\n",
    "        cosAngle = torch.cos(angle)\n",
    "        sinAngle = torch.sin(angle)\n",
    "        complementCos = 1 - cosAngle\n",
    "        x = axis[..., 0].unsqueeze(1)\n",
    "        y = axis[..., 1].unsqueeze(1)\n",
    "        z = axis[..., 2].unsqueeze(1)\n",
    "        xs = x * sinAngle\n",
    "        ys = y * sinAngle\n",
    "        zs = z * sinAngle\n",
    "        xcomplementCos = x * complementCos\n",
    "        ycomplementCos = y * complementCos\n",
    "        zcomplementCos = z * complementCos\n",
    "        xycomplementCos = x * ycomplementCos\n",
    "        yzcomplementCos = y * zcomplementCos\n",
    "        zxcomplementCos = z * xcomplementCos\n",
    "        rot = torch.zeros((axisangle.shape[0], 4, 4)).to(device=axisangle.device)\n",
    "        rot[:, 0, 0] = torch.squeeze(x * xcomplementCos + cosAngle)\n",
    "        rot[:, 0, 1] = torch.squeeze(xycomplementCos - zs)\n",
    "        rot[:, 0, 2] = torch.squeeze(zxcomplementCos + ys)\n",
    "        rot[:, 1, 0] = torch.squeeze(xycomplementCos + zs)\n",
    "        rot[:, 1, 1] = torch.squeeze(y * ycomplementCos + cosAngle)\n",
    "        rot[:, 1, 2] = torch.squeeze(yzcomplementCos - xs)\n",
    "        rot[:, 2, 0] = torch.squeeze(zxcomplementCos - ys)\n",
    "        rot[:, 2, 1] = torch.squeeze(yzcomplementCos + xs)\n",
    "        rot[:, 2, 2] = torch.squeeze(z * zcomplementCos + cosAngle)\n",
    "        rot[:, 3, 3] = 1\n",
    "        return rot\n",
    "\n",
    "    def getTranslationMatrix(self, translation):\n",
    "        T = torch.zeros(translation.shape[0], 4, 4).to(device=translation.device)\n",
    "        t = translation.contiguous().view(-1, 3, 1)\n",
    "        T[:, 0, 0] = 1\n",
    "        T[:, 1, 1] = 1\n",
    "        T[:, 2, 2] = 1\n",
    "        T[:, 3, 3] = 1\n",
    "        T[:, :3, 3, None] = t\n",
    "        return T\n",
    "\n",
    "    def transformParameters(self, axisangle, translation, invert=False):\n",
    "        rotation = self.rotationFromAxisAngle(axisangle)\n",
    "        trans = translation.clone()\n",
    "        if invert:\n",
    "            rotation = rotation.transpose(1, 2)\n",
    "            trans *= -1\n",
    "        T = self.getTranslationMatrix(trans)\n",
    "        if invert:\n",
    "            M = torch.matmul(rotation, T)\n",
    "        else:\n",
    "            M = torch.matmul(T, rotation)\n",
    "        return M\n",
    "\n",
    "    def predictPoses(self, inputs, features):\n",
    "        outputs = {}\n",
    "        poseFeatures = {fi: features[fi] for fi in self.frameIdxs}\n",
    "        for fi in self.frameIdxs[1:]:\n",
    "            if fi < 0:\n",
    "                poseInputs = [poseFeatures[fi], poseFeatures[0]]\n",
    "            else:\n",
    "                poseInputs = [poseFeatures[0], poseFeatures[fi]]\n",
    "            axisangle, translation = self.models[\"pose\"](poseInputs)\n",
    "            outputs[(\"axisangle\", 0, fi)] = axisangle\n",
    "            outputs[(\"translation\", 0, fi)] = translation\n",
    "            outputs[(\"cam_T_cam\", 0, fi)] = self.transformParameters(axisangle[:, 0], translation[:, 0], invert=(fi<0))\n",
    "        return outputs\n",
    "\n",
    "    def generateImagePredictions(self, inputs, outputs):\n",
    "        for scale in range(self.numScales):\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            disp = F.interpolate(disp, [self.height, self.width], mode=\"bilinear\",\n",
    "                                 align_corners=False)\n",
    "            sourceScale = 0\n",
    "            _, depth = self.dispToDepth(disp, 0.1, 100.0)\n",
    "            outputs[(\"depth\", 0, scale)] = depth\n",
    "            for i, frameIdx in enumerate(self.frameIdxs[1:]):\n",
    "                T = outputs[(\"cam_T_cam\", 0, frameIdx)]\n",
    "                cameraPoints = self.backprojectDepth[sourceScale](depth, inputs[(\"inv_K\", sourceScale)])\n",
    "                pixelCoordinates = self.project3d[sourceScale](cameraPoints, inputs[(\"K\", sourceScale)], T)\n",
    "                outputs[(\"sample\", frameIdx, scale)] = pixelCoordinates\n",
    "                outputs[(\"color\", frameIdx, scale)] = F.grid_sample(inputs[(\"color\", frameIdx, sourceScale)],\n",
    "                                                                    outputs[((\"sample\", frameIdx, scale))],\n",
    "                                                                    padding_mode=\"border\")\n",
    "                outputs[(\"color_identity\", frameIdx, scale)] = inputs[(\"color\", frameIdx, sourceScale)]\n",
    "\n",
    "    def computeDepthErrors(self, depthGroundTruth, depthPred):\n",
    "        threshold = torch.max((depthGroundTruth/depthPred), (depthPred/depthGroundTruth))\n",
    "        a1 = (threshold < 1.25).float().mean()\n",
    "        a2 = (threshold < 1.25**2).float().mean()\n",
    "        a3 = (threshold < 1.25**3).float().mean()\n",
    "        rootMeanSquaredError = (depthGroundTruth - depthPred)**2\n",
    "        rootMeanSquaredError = torch.sqrt(rootMeanSquaredError.mean())\n",
    "        rootMeanSquaredErrorLog = (torch.log(depthGroundTruth) - torch.log(depthPred))**2\n",
    "        rootMeanSquaredErrorLog = torch.sqrt(rootMeanSquaredErrorLog.mean())\n",
    "        absolute = torch.mean(torch.abs(depthGroundTruth - depthPred)/depthGroundTruth)\n",
    "        squared = torch.mean(((depthGroundTruth - depthPred)**2)/depthGroundTruth)\n",
    "        return absolute, squared, rootMeanSquaredError, rootMeanSquaredErrorLog, a1, a2, a3\n",
    "\n",
    "    def computeDepthLosses(self, inputs, outputs, losses):\n",
    "        depthPred = outputs[(\"depth\", 0, 0)]\n",
    "        depthPred = torch.clamp(F.interpolate(depthPred, [375, 1242], mode='bilinear',\n",
    "                                              align_corners=False), 1e-3, 80)\n",
    "        depthPred = depthPred.detach()\n",
    "        depthGroundTruth = inputs[\"depth_gt\"]\n",
    "        mask = depthGroundTruth > 0\n",
    "        cropMask = torch.zeros_like(mask)\n",
    "        cropMask[:, :, 153:371, 44:1197] = 1\n",
    "        mask = mask * cropMask\n",
    "        depthGroundTruth = depthGroundTruth[mask]\n",
    "        depthPred = depthPred[mask]\n",
    "        depthPred *= torch.median(depthGroundTruth)/torch.median(depthPred)\n",
    "        depthPred = torch.clamp(depthPred, 1e-3, 80)\n",
    "        depthErrors = self.computeDepthErrors(depthGroundTruth, depthPred)\n",
    "        for i, name in enumerate(self.depthMetricNames):\n",
    "            losses[name] = np.array(depthErrors[i].cpu())\n",
    "\n",
    "    def computeReprojectionLoss(self, pred, target):\n",
    "        absDiff = torch.abs(target - pred)\n",
    "        l1Loss = absDiff.mean(1, True)\n",
    "        ssim_loss = self.ssim(pred, target).mean(1, True)\n",
    "        return 0.85*ssim_loss + 0.15*l1Loss\n",
    "\n",
    "    def getSmoothLoss(self, disp, img):\n",
    "        gradientDispX = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "        gradientDispY = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "        gradientImgX = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "        gradientImgY = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "        gradientDispX *= torch.exp(-gradientImgX)\n",
    "        gradientDispY *= torch.exp(-gradientImgY)\n",
    "        return gradientDispX.mean() + gradientDispY.mean()\n",
    "\n",
    "    def computeLosses(self, inputs, outputs):\n",
    "        losses = {}\n",
    "        totalLoss = 0\n",
    "        for scale in range(self.numScales):\n",
    "            loss = 0\n",
    "            reprojectionLoss = []\n",
    "            sourceScale = 0\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            color = inputs[(\"color\", 0, scale)]\n",
    "            target = inputs[(\"color\", 0, sourceScale)]\n",
    "            for frameIdx in self.frameIdxs[1:]:\n",
    "                pred = outputs[(\"color\", frameIdx, scale)]\n",
    "                reprojectionLoss.append(self.computeReprojectionLoss(pred, target))\n",
    "            reprojectionLoss = torch.cat(reprojectionLoss, 1)\n",
    "            identityReprojectionLoss = []\n",
    "            for frameIdx in self.frameIdxs[1:]:\n",
    "                pred = inputs[(\"color\", frameIdx, sourceScale)]\n",
    "                identityReprojectionLoss.append(self.computeReprojectionLoss(pred, target))\n",
    "            identityReprojectionLoss = torch.cat(identityReprojectionLoss, 1)\n",
    "            identityReprojectionLoss += torch.randn(identityReprojectionLoss.shape, device=self.device) * 0.00001\n",
    "            combined = torch.cat((identityReprojectionLoss, reprojectionLoss), 1)\n",
    "            if combined.shape[1] == 1:\n",
    "                toOptimise = combined\n",
    "            else:\n",
    "                toOptimise, idxs = torch.min(combined, dim=1)\n",
    "            outputs[\"identity_selection/{}\".format(scale)] = (idxs > identityReprojectionLoss.shape[1] - 1).float()\n",
    "            loss += toOptimise.mean()\n",
    "            meanDisp = disp.mean(2, True).mean(3, True)\n",
    "            normDisp = disp / (meanDisp + 1e-7)\n",
    "            smoothLoss = self.getSmoothLoss(normDisp, color)\n",
    "            loss += (1e-3 * smoothLoss)/(2**scale)\n",
    "            totalLoss += loss\n",
    "            losses[\"loss/{}\".format(scale)] = loss\n",
    "        totalLoss /= self.numScales\n",
    "        losses[\"loss\"] = totalLoss\n",
    "        return losses\n",
    "\n",
    "    def processBatch(self, inputs):\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(self.device)\n",
    "        origScaleColorAug = torch.cat([inputs[(\"color_aug\", fi, 0)] for fi in self.frameIdxs])\n",
    "        allFrameFeatures = self.models[\"encoder\"](origScaleColorAug)\n",
    "        allFrameFeatures = [torch.split(f, self.batchSize) for f in allFrameFeatures]\n",
    "        features = {}\n",
    "        for i, frameIdx in enumerate(self.frameIdxs):\n",
    "            features[frameIdx] = [f[i] for f in allFrameFeatures]\n",
    "        outputs = self.models[\"decoder\"](features[0])\n",
    "        outputs.update(self.predictPoses(inputs, features))\n",
    "        self.generateImagePredictions(inputs, outputs)\n",
    "        losses = self.computeLosses(inputs, outputs)\n",
    "        return outputs, losses\n",
    "\n",
    "    def runEpoch(self):\n",
    "        self.setTrain()\n",
    "        for batchIdx, inputs in enumerate(self.trainLoader):\n",
    "            startTime = time.time()\n",
    "            outputs, losses = self.processBatch(inputs)\n",
    "            self.optimizer.zero_grad()\n",
    "            losses[\"loss\"].backward()\n",
    "            self.optimizer.step()\n",
    "            duration = time.time() - startTime\n",
    "            early_phase = batchIdx % 200 == 0 and self.step < 2000\n",
    "            late_phase = self.step % 1000 == 0\n",
    "            if early_phase or late_phase:\n",
    "                self.logTime(batchIdx, duration, losses[\"loss\"].cpu().data)\n",
    "                self.computeDepthLosses(inputs, outputs, losses)\n",
    "                self.log(\"train\", inputs, outputs, losses)\n",
    "                self.val()\n",
    "            self.step += 1\n",
    "        self.lrScheduler.step()\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Total Trainable Parameters : {}\".format(self.totalTrainableParams))\n",
    "        print(\"Total Steps : {}\".format(self.numSteps))\n",
    "        self.epoch = 0\n",
    "        self.step = 0\n",
    "        self.startTime = time.time()\n",
    "        for self.epoch in range(self.epochs):\n",
    "            print(\"Training --- Epoch : {}\".format(self.epoch))\n",
    "            self.runEpoch()\n",
    "            self.saveModel()\n",
    "\n",
    "    def val(self):\n",
    "        self.setEval()\n",
    "        try:\n",
    "            inputs = self.valIterator.next()\n",
    "        except:\n",
    "            self.valIterator = iter(self.valLoader)\n",
    "            inputs = self.valIterator.next()\n",
    "        with torch.no_grad():\n",
    "            outputs, losses = self.processBatch(inputs)\n",
    "            self.computeDepthLosses(inputs, outputs, losses)\n",
    "            self.log(\"val\", inputs, outputs, losses)\n",
    "            del inputs, outputs, losses\n",
    "        self.setTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "t = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters : 34696354\n",
      "Total Steps : 39800\n",
      "Training --- Epoch : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mp6021/.local/lib/python3.8/site-packages/torch/nn/functional.py:4193: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :   0 | Batch :       0 | Step :          0 | examples/s:   6.9 | loss : 0.15653 | time elapsed: 00h00m11s | time left: 00h00m00s\n",
      "Epoch :   0 | Batch :     200 | Step :        200 | examples/s:   8.3 | loss : 0.13675 | time elapsed: 00h08m12s | time left: 27h06m36s\n",
      "Epoch :   0 | Batch :     400 | Step :        400 | examples/s:   8.4 | loss : 0.13278 | time elapsed: 00h16m13s | time left: 26h38m49s\n",
      "Epoch :   0 | Batch :     600 | Step :        600 | examples/s:   8.4 | loss : 0.12266 | time elapsed: 00h24m14s | time left: 26h24m06s\n",
      "Epoch :   1 | Batch :    1010 | Step :       3000 | examples/s:   8.3 | loss : 0.12210 | time elapsed: 02h00m43s | time left: 24h40m49s\n",
      "Training --- Epoch : 2\n",
      "Epoch :   2 | Batch :      20 | Step :       4000 | examples/s:   8.3 | loss : 0.12282 | time elapsed: 02h40m57s | time left: 24h00m33s\n",
      "Epoch :   2 | Batch :    1020 | Step :       5000 | examples/s:   8.3 | loss : 0.11518 | time elapsed: 03h21m03s | time left: 23h19m19s\n",
      "Training --- Epoch : 7\n",
      "Training --- Epoch : 8\n",
      "Epoch :   8 | Batch :      80 | Step :      16000 | examples/s:   8.3 | loss : 0.08534 | time elapsed: 10h43m03s | time left: 15h56m32s\n"
     ]
    }
   ],
   "source": [
    "t.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
