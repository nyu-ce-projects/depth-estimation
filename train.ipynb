{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: torch in /home/mp6021/.local/lib/python3.8/site-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision in /home/mp6021/.local/lib/python3.8/site-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: torchaudio in /home/mp6021/.local/lib/python3.8/site-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /home/mp6021/.local/lib/python3.8/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: numpy in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: requests in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torchvision) (2.24.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (2.10)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorboardX in /home/mp6021/.local/lib/python3.8/site-packages (2.5)\n",
      "Requirement already satisfied: six in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from tensorboardX) (1.15.0)\n",
      "Requirement already satisfied: numpy in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from tensorboardX) (1.19.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/mp6021/.local/lib/python3.8/site-packages (from tensorboardX) (3.19.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-image in /home/mp6021/.local/lib/python3.8/site-packages (0.19.2)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/mp6021/.local/lib/python3.8/site-packages (from scikit-image) (2.16.1)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from scikit-image) (8.0.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/scipy-1.5.2-py3.8-linux-x86_64.egg (from scikit-image) (1.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from scikit-image) (20.4)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/mp6021/.local/lib/python3.8/site-packages (from scikit-image) (1.3.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/mp6021/.local/lib/python3.8/site-packages (from scikit-image) (2.7.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/mp6021/.local/lib/python3.8/site-packages (from scikit-image) (2022.3.25)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from scikit-image) (1.19.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from packaging>=20.0->scikit-image) (2.4.7)\n",
      "Requirement already satisfied: six in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from packaging>=20.0->scikit-image) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from Models.EncoderModel import EncoderModel, MultiImageEncoderModel\n",
    "from Models.DecoderModel import DepthDecoderModel, PoseDecoderModel\n",
    "from Models.BackprojectDepth import BackprojectDepth\n",
    "from Models.Project3D import Project3D\n",
    "from Dataset.KITTI import KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, LR=0.001, batchSize=24, epochs=20, height=192, width=640, frameIdxs=[0, -1, 1],\n",
    "                 scales=[0, 1, 2, 3]):\n",
    "        self.LR = LR\n",
    "        self.batchSize = batchSize\n",
    "        self.epochs = epochs\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.frameIdxs = frameIdxs\n",
    "        self.numScales = len(scales)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        self.models = {}\n",
    "        self.totalTrainableParams = 0\n",
    "        self.trainableParameters = []\n",
    "        self.models[\"encoder\"] = EncoderModel(50)\n",
    "        self.models[\"encoder\"] = self.models[\"encoder\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"encoder\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"encoder\"].parameters() if p.requires_grad)\n",
    "        self.models[\"decoder\"] = DepthDecoderModel(self.models[\"encoder\"].numChannels)\n",
    "        self.models[\"decoder\"] = self.models[\"decoder\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"decoder\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"decoder\"].parameters() if p.requires_grad)\n",
    "        self.models[\"pose_encoder\"] = MultiImageEncoderModel(50)\n",
    "        self.models[\"pose_encoder\"] = self.models[\"pose_encoder\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"pose_encoder\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"pose_encoder\"].parameters() if p.requires_grad)\n",
    "        self.models[\"pose\"] = PoseDecoderModel(self.models[\"pose_encoder\"].numChannels)\n",
    "        self.models[\"pose\"] = self.models[\"pose\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"pose\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"pose\"].parameters() if p.requires_grad)\n",
    "        self.optimizer = optim.Adam(self.trainableParameters, lr=self.LR)\n",
    "        self.lrScheduler = optim.lr_scheduler.StepLR(self.optimizer, 15, 0.1)\n",
    "        self.loadDataset()\n",
    "        self.depthMetricNames = [\"de/abs_rel\", \"de/sq_rel\", \"de/rms\", \"de/log_rms\", \"da/a1\", \"da/a2\", \"da/a3\"]\n",
    "        self.backprojectDepth = {}\n",
    "        self.project3d = {}\n",
    "        for scale in range(self.numScales):\n",
    "            h = self.height // (2**scale)\n",
    "            w = self.width // (2**scale)\n",
    "            self.backprojectDepth[scale] = BackprojectDepth(self.batchSize, h, w)\n",
    "            self.backprojectDepth[scale] = self.backprojectDepth[scale].to(self.device)\n",
    "            self.project3d[scale] = Project3D(self.batchSize, h, w)\n",
    "            self.project3d[scale] = self.project3d[scale].to(self.device)\n",
    "        self.writers = {}\n",
    "        for mode in [\"train\", \"val\"]:\n",
    "            self.writers[mode] = SummaryWriter(os.path.join(\"/scratch/mp6021/Monodepth2/logs\", mode))\n",
    "\n",
    "    def readlines(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "        return lines\n",
    "\n",
    "    def loadDataset(self):\n",
    "        self.dataset = KITTI\n",
    "        dataPath = os.path.join(\"/scratch/mp6021/Monodepth2\", \"data\", \"KITTI\")\n",
    "        filepath = os.path.join(dataPath, \"splits\", \"eigen_zhou\", \"{}_files.txt\")\n",
    "        trainFilenames = self.readlines(filepath.format(\"train\"))\n",
    "        valFilenames = self.readlines(filepath.format(\"val\"))\n",
    "        numTrain = len(trainFilenames)\n",
    "        self.numSteps = (numTrain//self.batchSize)*self.epochs\n",
    "        trainDataset = self.dataset(dataPath, trainFilenames, self.height, self.width,\n",
    "                                    self.frameIdxs, 4, True)\n",
    "        valDataset = self.dataset(dataPath, valFilenames, self.height, self.width, self.frameIdxs,\n",
    "                                  4, False)\n",
    "        self.trainLoader = DataLoader(trainDataset, self.batchSize, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "        self.valLoader = DataLoader(valDataset, self.batchSize, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "        self.valIterator = iter(self.valLoader)\n",
    "\n",
    "    def setTrain(self):\n",
    "        for model in self.models.values():\n",
    "            model.train()\n",
    "\n",
    "    def setEval(self):\n",
    "        for model in self.models.values():\n",
    "            model.eval()\n",
    "            \n",
    "    def log(self, mode, inputs, outputs, losses):\n",
    "        writer = self.writers[mode]\n",
    "        for lossname, value in losses.items():\n",
    "            writer.add_scalar(\"{}\".format(lossname), value, self.step)\n",
    "        for i in range(4):\n",
    "            for scale in range(self.numScales):\n",
    "                for frameIdx in self.frameIdxs:\n",
    "                    writer.add_image(\"color_{}_{}/{}\".format(frameIdx, scale, i), inputs[(\"color\", frameIdx, scale)][i].data, self.step)\n",
    "                    if scale == 0 and frameIdx != 0:\n",
    "                        writer.add_image(\"color_pred_{}_{}/{}\".format(frameIdx, scale, i), outputs[(\"color\", frameIdx, scale)][i].data, self.step)\n",
    "                writer.add_image(\"disp_{}/{}\".format(scale, i), self.normalizeImage(outputs[(\"disp\", scale)][i]), self.step)\n",
    "\n",
    "    def saveModel(self):\n",
    "        outpath = os.path.join(\"/scratch/mp6021/Monodepth2\", \"models\", \"weights_{}\".format(self.epoch))\n",
    "        if not os.path.exists(outpath):\n",
    "            os.makedirs(outpath)\n",
    "        for name, model in self.models.items():\n",
    "            savePath = os.path.join(outpath, \"{}.pth\".format(name))\n",
    "            toSave = model.state_dict()\n",
    "            if name == \"encoder\":\n",
    "                toSave[\"height\"] = self.height\n",
    "                toSave[\"width\"] = self.width\n",
    "            torch.save(toSave, savePath)\n",
    "        savePath = os.path.join(outpath, \"adam.pth\")\n",
    "        torch.save(self.optimizer.state_dict(), savePath)\n",
    "        \n",
    "    def normalizeImage(self, image):\n",
    "        maxValue = float(image.max().cpu().data)\n",
    "        minValue = float(image.min().cpu().data)\n",
    "        diff = (maxValue - minValue) if maxValue != minValue else 1e5\n",
    "        return (image - minValue)/diff\n",
    "\n",
    "    def dispToDepth(self, disp, minDepth, maxDepth):\n",
    "        minDisp = 1 / maxDepth\n",
    "        maxDisp = 1 / minDepth\n",
    "        scaledDisp = minDisp + (maxDisp - minDisp)*disp\n",
    "        depth = 1 / scaledDisp\n",
    "        return scaledDisp, depth\n",
    "\n",
    "    def rotationFromAxisAngle(self, axisangle):\n",
    "        angle = torch.norm(axisangle, 2, 2, True)\n",
    "        axis = axisangle / (angle + 1e-7)\n",
    "        cosAngle = torch.cos(angle)\n",
    "        sinAngle = torch.sin(angle)\n",
    "        complementCos = 1 - cosAngle\n",
    "        x = axis[..., 0].unsqueeze(1)\n",
    "        y = axis[..., 1].unsqueeze(1)\n",
    "        z = axis[..., 2].unsqueeze(1)\n",
    "        xs = x * sinAngle\n",
    "        ys = y * sinAngle\n",
    "        zs = z * sinAngle\n",
    "        xcomplementCos = x * complementCos\n",
    "        ycomplementCos = y * complementCos\n",
    "        zcomplementCos = z * complementCos\n",
    "        xycomplementCos = x * ycomplementCos\n",
    "        yzcomplementCos = y * zcomplementCos\n",
    "        zxcomplementCos = z * xcomplementCos\n",
    "        rot = torch.zeros((axisangle.shape[0], 4, 4)).to(device=axisangle.device)\n",
    "        rot[:, 0, 0] = torch.squeeze(x * xcomplementCos + cosAngle)\n",
    "        rot[:, 0, 1] = torch.squeeze(xycomplementCos - zs)\n",
    "        rot[:, 0, 2] = torch.squeeze(zxcomplementCos + ys)\n",
    "        rot[:, 1, 0] = torch.squeeze(xycomplementCos + zs)\n",
    "        rot[:, 1, 1] = torch.squeeze(y * ycomplementCos + cosAngle)\n",
    "        rot[:, 1, 2] = torch.squeeze(yzcomplementCos - xs)\n",
    "        rot[:, 2, 0] = torch.squeeze(zxcomplementCos - ys)\n",
    "        rot[:, 2, 1] = torch.squeeze(yzcomplementCos + xs)\n",
    "        rot[:, 2, 2] = torch.squeeze(z * zcomplementCos + cosAngle)\n",
    "        rot[:, 3, 3] = 1\n",
    "        return rot\n",
    "\n",
    "    def getTranslationMatrix(self, translation):\n",
    "        T = torch.zeros(translation.shape[0], 4, 4).to(device=translation.device)\n",
    "        t = translation.contiguous().view(-1, 3, 1)\n",
    "        T[:, 0, 0] = 1\n",
    "        T[:, 1, 1] = 1\n",
    "        T[:, 2, 2] = 1\n",
    "        T[:, 3, 3] = 1\n",
    "        T[:, :3, 3, None] = t\n",
    "        return T\n",
    "\n",
    "    def transformParameters(self, axisangle, translation, invert=False):\n",
    "        rotation = self.rotationFromAxisAngle(axisangle)\n",
    "        trans = translation.clone()\n",
    "        if invert:\n",
    "            rotation = rotation.transpose(1, 2)\n",
    "            trans *= -1\n",
    "        T = self.getTranslationMatrix(trans)\n",
    "        if invert:\n",
    "            M = torch.matmul(rotation, T)\n",
    "        else:\n",
    "            M = torch.matmul(T, rotation)\n",
    "        return M\n",
    "\n",
    "    def predictPoses(self, inputs, features):\n",
    "        outputs = {}\n",
    "        poseFeatures = {fi: inputs[\"color_aug\", fi, 0] for fi in self.frameIdxs}\n",
    "        for fi in self.frameIdxs[1:]:\n",
    "            if fi < 0:\n",
    "                poseInputs = [poseFeatures[fi], poseFeatures[0]]\n",
    "            else:\n",
    "                poseInputs = [poseFeatures[0], poseFeatures[fi]]\n",
    "            poseInputs = [self.models[\"pose_encoder\"](torch.cat(poseInputs, 1))]\n",
    "            axisangle, translation = self.models[\"pose\"](poseInputs)\n",
    "            outputs[(\"axisangle\", 0, fi)] = axisangle\n",
    "            outputs[(\"translation\", 0, fi)] = translation\n",
    "            outputs[(\"cam_T_cam\", 0, fi)] = self.transformParameters(axisangle[:, 0], translation[:, 0], invert=(fi<0))\n",
    "        return outputs\n",
    "\n",
    "    def generateImagePredictions(self, inputs, outputs):\n",
    "        for scale in range(self.numScales):\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            disp = F.interpolate(disp, [self.height, self.width], mode=\"bilinear\",\n",
    "                                 align_corners=False)\n",
    "            sourceScale = 0\n",
    "            _, depth = self.dispToDepth(disp, 0.1, 100.0)\n",
    "            outputs[(\"depth\", 0, scale)] = depth\n",
    "            for i, frameIdx in enumerate(self.frameIdxs[1:]):\n",
    "                T = outputs[(\"cam_T_cam\", 0, frameIdx)]\n",
    "                cameraPoints = self.backprojectDepth[sourceScale](depth, inputs[(\"inv_K\", sourceScale)])\n",
    "                pixelCoordinates = self.project3d[sourceScale](cameraPoints, inputs[(\"K\", sourceScale)], T)\n",
    "                outputs[(\"sample\", frameIdx, scale)] = pixelCoordinates\n",
    "                outputs[(\"color\", frameIdx, scale)] = F.grid_sample(inputs[(\"color\", frameIdx, sourceScale)],\n",
    "                                                                    outputs[((\"sample\", frameIdx, scale))],\n",
    "                                                                    padding_mode=\"border\")\n",
    "\n",
    "    def computeDepthErrors(self, depthGroundTruth, depthPred):\n",
    "        threshold = torch.max((depthGroundTruth/depthPred), (depthPred/depthGroundTruth))\n",
    "        a1 = (threshold < 1.25).float().mean()\n",
    "        a2 = (threshold < 1.25**2).float().mean()\n",
    "        a3 = (threshold < 1.25**3).float().mean()\n",
    "        rootMeanSquaredError = (depthGroundTruth - depthPred)**2\n",
    "        rootMeanSquaredError = torch.sqrt(rootMeanSquaredError.mean())\n",
    "        rootMeanSquaredErrorLog = (torch.log(depthGroundTruth) - torch.log(depthPred))**2\n",
    "        rootMeanSquaredErrorLog = torch.sqrt(rootMeanSquaredErrorLog.mean())\n",
    "        absolute = torch.mean(torch.abs(depthGroundTruth - depthPred)/depthGroundTruth)\n",
    "        squared = torch.mean(((depthGroundTruth - depthPred)**2)/depthGroundTruth)\n",
    "        return absolute, squared, rootMeanSquaredError, rootMeanSquaredErrorLog, a1, a2, a3\n",
    "\n",
    "    def computeDepthLosses(self, inputs, outputs, losses):\n",
    "        depthPred = outputs[(\"depth\", 0, 0)]\n",
    "        depthPred = torch.clamp(F.interpolate(depthPred, [375, 1242], mode='bilinear',\n",
    "                                              align_corners=False), 1e-3, 80)\n",
    "        depthPred = depthPred.detach()\n",
    "        depthGroundTruth = inputs[\"depth_gt\"]\n",
    "        mask = depthGroundTruth > 0\n",
    "        cropMask = torch.zeros_like(mask)\n",
    "        cropMask[:, :, 153:371, 44:1197] = 1\n",
    "        mask = mask * cropMask\n",
    "        depthGroundTruth = depthGroundTruth[mask]\n",
    "        depthPred = depthPred[mask]\n",
    "        depthPred *= torch.median(depthGroundTruth)/torch.median(depthPred)\n",
    "        depthPred = torch.clamp(depthPred, 1e-3, 80)\n",
    "        depthErrors = self.computeDepthErrors(depthGroundTruth, depthPred)\n",
    "        for i, name in enumerate(self.depthMetricNames):\n",
    "            losses[name] = np.array(depthErrors[i].cpu())\n",
    "\n",
    "    def computeReprojectionLoss(self, pred, target):\n",
    "        absDiff = torch.abs(pred - target)\n",
    "        l1Loss = absDiff.mean(1, True)\n",
    "        return l1Loss\n",
    "\n",
    "    def getSmoothLoss(self, disp, img):\n",
    "        gradientDispX = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "        gradientDispY = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "        gradientImgX = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "        gradientImgY = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "        gradientDispX *= torch.exp(-gradientImgX)\n",
    "        gradientDispY *= torch.exp(-gradientImgY)\n",
    "        return gradientDispX.mean() + gradientDispY.mean()\n",
    "\n",
    "    def computeLosses(self, inputs, outputs):\n",
    "        losses = {}\n",
    "        totalLoss = 0\n",
    "        for scale in range(self.numScales):\n",
    "            loss = 0\n",
    "            reprojectionLoss = []\n",
    "            sourceScale = 0\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            color = inputs[(\"color\", 0, scale)]\n",
    "            target = inputs[(\"color\", 0, sourceScale)]\n",
    "            for frameIdx in self.frameIdxs[1:]:\n",
    "                pred = outputs[(\"color\", frameIdx, scale)]\n",
    "                reprojectionLoss.append(self.computeReprojectionLoss(pred, target))\n",
    "            reprojectionLoss = torch.cat(reprojectionLoss, 1)\n",
    "            combined = reprojectionLoss\n",
    "            if combined.shape[1] == 1:\n",
    "                toOptimise = combined\n",
    "            else:\n",
    "                toOptimise, idxs = torch.min(combined, dim=1)\n",
    "            loss += toOptimise.mean()\n",
    "            meanDisp = disp.mean(2, True).mean(3, True)\n",
    "            normDisp = disp / (meanDisp + 1e-7)\n",
    "            smoothLoss = self.getSmoothLoss(normDisp, color)\n",
    "            loss += (1e-3 * smoothLoss)/(2**scale)\n",
    "            totalLoss += loss\n",
    "            losses[\"loss/{}\".format(scale)] = loss\n",
    "        totalLoss /= self.numScales\n",
    "        losses[\"loss\"] = totalLoss\n",
    "        return losses\n",
    "\n",
    "    def processBatch(self, inputs):\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(self.device)\n",
    "        features = self.models[\"encoder\"](inputs[\"color_aug\", 0, 0])\n",
    "        outputs = self.models[\"decoder\"](features)\n",
    "        outputs.update(self.predictPoses(inputs, features))\n",
    "        self.generateImagePredictions(inputs, outputs)\n",
    "        losses = self.computeLosses(inputs, outputs)\n",
    "        return outputs, losses\n",
    "\n",
    "    def runEpoch(self):\n",
    "        self.lrScheduler.step()\n",
    "        self.setTrain()\n",
    "        for batchIdx, inputs in enumerate(self.trainLoader):\n",
    "            outputs, losses = self.processBatch(inputs)\n",
    "            self.optimizer.zero_grad()\n",
    "            losses[\"loss\"].backward()\n",
    "            self.optimizer.step()\n",
    "            self.computeDepthLosses(inputs, outputs, losses)\n",
    "            self.log(\"train\", inputs, outputs, losses)\n",
    "            if (self.step + 1)%20 == 0:\n",
    "                print(\"Epoch : {}, Batch : {}, Loss : {}\".format(self.epoch, batchIdx, losses[\"loss\"].item()))\n",
    "            self.val()\n",
    "            self.step += 1\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Total Trainable Parameters : {}\".format(self.totalTrainableParams))\n",
    "        print(\"Total Steps : {}\".format(self.numSteps))\n",
    "        self.epoch = 0\n",
    "        self.step = 0\n",
    "        for self.epoch in range(self.epochs):\n",
    "            print(\"Training --- Epoch : {}\".format(self.epoch))\n",
    "            self.runEpoch()\n",
    "            if (self.epoch + 1) % 5 == 0:\n",
    "                self.saveModel()\n",
    "\n",
    "    def val(self):\n",
    "        self.setEval()\n",
    "        try:\n",
    "            inputs = self.valIterator.next()\n",
    "        except:\n",
    "            self.valIterator = iter(self.valLoader)\n",
    "            inputs = self.valIterator.next()\n",
    "        with torch.no_grad():\n",
    "            outputs, losses = self.processBatch(inputs)\n",
    "            self.computeDepthLosses(inputs, outputs, losses)\n",
    "            self.log(\"val\", inputs, outputs, losses)\n",
    "            del inputs, outputs, losses\n",
    "        self.setTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mp6021/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters : 61845360\n",
      "Total Steps : 33160\n",
      "Training --- Epoch : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mp6021/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/mp6021/.local/lib/python3.8/site-packages/torch/nn/functional.py:4193: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Batch : 19, Loss : 0.047032441943883896\n",
      "Epoch : 0, Batch : 39, Loss : 0.051038648933172226\n"
     ]
    }
   ],
   "source": [
    "t.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
