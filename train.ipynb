{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorboardX\n",
    "#!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from Models.EncoderModel import EncoderModel\n",
    "from Models.DecoderModel import DepthDecoderModel, PoseDecoderModel\n",
    "from Losses.SSIM import SSIM\n",
    "from Models.BackprojectDepth import BackprojectDepth\n",
    "from Models.Project3D import Project3D\n",
    "from Dataset.KITTI import KITTI\n",
    "from utils import secondsToHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, LR=0.0001, batchSize=24, epochs=20, height=192, width=640, frameIdxs=[0, -1, 1],\n",
    "                 scales=[0, 1, 2, 3]):\n",
    "        self.LR = LR\n",
    "        self.batchSize = batchSize\n",
    "        self.epochs = epochs\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.frameIdxs = frameIdxs\n",
    "        self.numScales = len(scales)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        self.models = {}\n",
    "        self.totalTrainableParams = 0\n",
    "        self.trainableParameters = []\n",
    "        self.models[\"encoder\"] = EncoderModel(50)\n",
    "        self.models[\"encoder\"] = self.models[\"encoder\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"encoder\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"encoder\"].parameters() if p.requires_grad)\n",
    "        self.models[\"decoder\"] = DepthDecoderModel(self.models[\"encoder\"].numChannels)\n",
    "        self.models[\"decoder\"] = self.models[\"decoder\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"decoder\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"decoder\"].parameters() if p.requires_grad)\n",
    "        #self.models[\"pose_encoder\"] = MultiImageEncoderModel(50)\n",
    "        #self.models[\"pose_encoder\"] = self.models[\"pose_encoder\"].to(self.device)\n",
    "        #self.trainableParameters += list(self.models[\"pose_encoder\"].parameters())\n",
    "        #self.totalTrainableParams += sum(p.numel() for p in self.models[\"pose_encoder\"].parameters() if p.requires_grad)\n",
    "        self.models[\"pose\"] = PoseDecoderModel(self.models[\"encoder\"].numChannels)\n",
    "        self.models[\"pose\"] = self.models[\"pose\"].to(self.device)\n",
    "        self.trainableParameters += list(self.models[\"pose\"].parameters())\n",
    "        self.totalTrainableParams += sum(p.numel() for p in self.models[\"pose\"].parameters() if p.requires_grad)\n",
    "        self.ssim = SSIM()\n",
    "        self.ssim = self.ssim.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.trainableParameters, lr=self.LR)\n",
    "        self.lrScheduler = optim.lr_scheduler.StepLR(self.optimizer, 15, 0.1)\n",
    "        self.loadDataset()\n",
    "        self.depthMetricNames = [\"de/abs_rel\", \"de/sq_rel\", \"de/rms\", \"de/log_rms\", \"da/a1\", \"da/a2\", \"da/a3\"]\n",
    "        self.backprojectDepth = {}\n",
    "        self.project3d = {}\n",
    "        for scale in range(self.numScales):\n",
    "            h = self.height // (2**scale)\n",
    "            w = self.width // (2**scale)\n",
    "            self.backprojectDepth[scale] = BackprojectDepth(self.batchSize, h, w)\n",
    "            self.backprojectDepth[scale] = self.backprojectDepth[scale].to(self.device)\n",
    "            self.project3d[scale] = Project3D(self.batchSize, h, w)\n",
    "            self.project3d[scale] = self.project3d[scale].to(self.device)\n",
    "        self.writers = {}\n",
    "        for mode in [\"train\", \"val\"]:\n",
    "            self.writers[mode] = SummaryWriter(os.path.join(\"/scratch/{}/Monodepth2/logs\".format(os.environ['LOGNAME']), mode))\n",
    "\n",
    "    def readlines(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "        return lines\n",
    "\n",
    "    def loadDataset(self):\n",
    "        self.dataset = KITTI\n",
    "        dataPath = os.path.join(\"/scratch/mp6021/Monodepth2\", \"data\", \"KITTI\")\n",
    "        filepath = os.path.join(dataPath, \"splits\", \"eigen_zhou\", \"{}_files.txt\")\n",
    "        trainFilenames = self.readlines(filepath.format(\"train\"))\n",
    "        valFilenames = self.readlines(filepath.format(\"val\"))\n",
    "        numTrain = len(trainFilenames)\n",
    "        self.numSteps = (numTrain//self.batchSize)*self.epochs\n",
    "        trainDataset = self.dataset(dataPath, trainFilenames, self.height, self.width,\n",
    "                                    self.frameIdxs, 4, True)\n",
    "        valDataset = self.dataset(dataPath, valFilenames, self.height, self.width, self.frameIdxs,\n",
    "                                  4, False)\n",
    "        self.trainLoader = DataLoader(trainDataset, self.batchSize, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "        self.valLoader = DataLoader(valDataset, self.batchSize, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "        self.valIterator = iter(self.valLoader)\n",
    "\n",
    "    def setTrain(self):\n",
    "        for model in self.models.values():\n",
    "            model.train()\n",
    "\n",
    "    def setEval(self):\n",
    "        for model in self.models.values():\n",
    "            model.eval()\n",
    "            \n",
    "    def log(self, mode, inputs, outputs, losses):\n",
    "        writer = self.writers[mode]\n",
    "        for lossname, value in losses.items():\n",
    "            writer.add_scalar(\"{}\".format(lossname), value, self.step)\n",
    "        for i in range(4):\n",
    "            for frameIdx in self.frameIdxs:\n",
    "                writer.add_image(\"color_{}/{}\".format(frameIdx, i), inputs[(\"color\", frameIdx, 0)][i].data, self.step)\n",
    "                if frameIdx != 0:\n",
    "                    writer.add_image(\"color_pred_{}/{}\".format(frameIdx, i), outputs[(\"color\", frameIdx, 0)][i].data, self.step)\n",
    "                writer.add_image(\"disp/{}\".format(i), self.normalizeImage(outputs[(\"disp\", 0)][i]), self.step)\n",
    "    \n",
    "    def logTime(self, batchIdx, duration, loss):\n",
    "        samplesPerSec = self.batchSize / duration\n",
    "        totalTime = time.time() - self.startTime\n",
    "        timeLeft = (self.numSteps / self.step - 1.0)*totalTime if self.step > 0 else 0\n",
    "        logString = \"Epoch : {:>3} | Batch : {:>7}, examples/s: {:5.1f}, loss : {:.5f}, time elapsed: {}, time left: {}\"\n",
    "        print(logString.format(self.epoch, batchIdx, samplesPerSec, loss, secondsToHM(totalTime), secondsToHM(timeLeft)))\n",
    "\n",
    "    def saveModel(self):\n",
    "        outpath = os.path.join(\"/scratch/mp6021/Monodepth2\", \"models\", \"weights_{}\".format(self.epoch))\n",
    "        if not os.path.exists(outpath):\n",
    "            os.makedirs(outpath)\n",
    "        for name, model in self.models.items():\n",
    "            savePath = os.path.join(outpath, \"{}.pth\".format(name))\n",
    "            toSave = model.state_dict()\n",
    "            if name == \"encoder\":\n",
    "                toSave[\"height\"] = self.height\n",
    "                toSave[\"width\"] = self.width\n",
    "            torch.save(toSave, savePath)\n",
    "        savePath = os.path.join(outpath, \"adam.pth\")\n",
    "        torch.save(self.optimizer.state_dict(), savePath)\n",
    "        \n",
    "    def normalizeImage(self, image):\n",
    "        maxValue = float(image.max().cpu().data)\n",
    "        minValue = float(image.min().cpu().data)\n",
    "        diff = (maxValue - minValue) if maxValue != minValue else 1e5\n",
    "        return (image - minValue)/diff\n",
    "\n",
    "    def dispToDepth(self, disp, minDepth, maxDepth):\n",
    "        minDisp = 1 / maxDepth\n",
    "        maxDisp = 1 / minDepth\n",
    "        scaledDisp = minDisp + (maxDisp - minDisp)*disp\n",
    "        depth = 1 / scaledDisp\n",
    "        return scaledDisp, depth\n",
    "\n",
    "    def rotationFromAxisAngle(self, axisangle):\n",
    "        angle = torch.norm(axisangle, 2, 2, True)\n",
    "        axis = axisangle / (angle + 1e-7)\n",
    "        cosAngle = torch.cos(angle)\n",
    "        sinAngle = torch.sin(angle)\n",
    "        complementCos = 1 - cosAngle\n",
    "        x = axis[..., 0].unsqueeze(1)\n",
    "        y = axis[..., 1].unsqueeze(1)\n",
    "        z = axis[..., 2].unsqueeze(1)\n",
    "        xs = x * sinAngle\n",
    "        ys = y * sinAngle\n",
    "        zs = z * sinAngle\n",
    "        xcomplementCos = x * complementCos\n",
    "        ycomplementCos = y * complementCos\n",
    "        zcomplementCos = z * complementCos\n",
    "        xycomplementCos = x * ycomplementCos\n",
    "        yzcomplementCos = y * zcomplementCos\n",
    "        zxcomplementCos = z * xcomplementCos\n",
    "        rot = torch.zeros((axisangle.shape[0], 4, 4)).to(device=axisangle.device)\n",
    "        rot[:, 0, 0] = torch.squeeze(x * xcomplementCos + cosAngle)\n",
    "        rot[:, 0, 1] = torch.squeeze(xycomplementCos - zs)\n",
    "        rot[:, 0, 2] = torch.squeeze(zxcomplementCos + ys)\n",
    "        rot[:, 1, 0] = torch.squeeze(xycomplementCos + zs)\n",
    "        rot[:, 1, 1] = torch.squeeze(y * ycomplementCos + cosAngle)\n",
    "        rot[:, 1, 2] = torch.squeeze(yzcomplementCos - xs)\n",
    "        rot[:, 2, 0] = torch.squeeze(zxcomplementCos - ys)\n",
    "        rot[:, 2, 1] = torch.squeeze(yzcomplementCos + xs)\n",
    "        rot[:, 2, 2] = torch.squeeze(z * zcomplementCos + cosAngle)\n",
    "        rot[:, 3, 3] = 1\n",
    "        return rot\n",
    "\n",
    "    def getTranslationMatrix(self, translation):\n",
    "        T = torch.zeros(translation.shape[0], 4, 4).to(device=translation.device)\n",
    "        t = translation.contiguous().view(-1, 3, 1)\n",
    "        T[:, 0, 0] = 1\n",
    "        T[:, 1, 1] = 1\n",
    "        T[:, 2, 2] = 1\n",
    "        T[:, 3, 3] = 1\n",
    "        T[:, :3, 3, None] = t\n",
    "        return T\n",
    "\n",
    "    def transformParameters(self, axisangle, translation, invert=False):\n",
    "        rotation = self.rotationFromAxisAngle(axisangle)\n",
    "        trans = translation.clone()\n",
    "        if invert:\n",
    "            rotation = rotation.transpose(1, 2)\n",
    "            trans *= -1\n",
    "        T = self.getTranslationMatrix(trans)\n",
    "        if invert:\n",
    "            M = torch.matmul(rotation, T)\n",
    "        else:\n",
    "            M = torch.matmul(T, rotation)\n",
    "        return M\n",
    "\n",
    "    def predictPoses(self, inputs, features):\n",
    "        outputs = {}\n",
    "        poseFeatures = {fi: features[fi] for fi in self.frameIdxs}\n",
    "        for fi in self.frameIdxs[1:]:\n",
    "            if fi < 0:\n",
    "                poseInputs = [poseFeatures[fi], poseFeatures[0]]\n",
    "            else:\n",
    "                poseInputs = [poseFeatures[0], poseFeatures[fi]]\n",
    "            axisangle, translation = self.models[\"pose\"](poseInputs)\n",
    "            outputs[(\"axisangle\", 0, fi)] = axisangle\n",
    "            outputs[(\"translation\", 0, fi)] = translation\n",
    "            outputs[(\"cam_T_cam\", 0, fi)] = self.transformParameters(axisangle[:, 0], translation[:, 0], invert=(fi<0))\n",
    "        return outputs\n",
    "\n",
    "    def generateImagePredictions(self, inputs, outputs):\n",
    "        for scale in range(self.numScales):\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            disp = F.interpolate(disp, [self.height, self.width], mode=\"bilinear\",\n",
    "                                 align_corners=False)\n",
    "            sourceScale = 0\n",
    "            _, depth = self.dispToDepth(disp, 0.1, 100.0)\n",
    "            outputs[(\"depth\", 0, scale)] = depth\n",
    "            for i, frameIdx in enumerate(self.frameIdxs[1:]):\n",
    "                T = outputs[(\"cam_T_cam\", 0, frameIdx)]\n",
    "                cameraPoints = self.backprojectDepth[sourceScale](depth, inputs[(\"inv_K\", sourceScale)])\n",
    "                pixelCoordinates = self.project3d[sourceScale](cameraPoints, inputs[(\"K\", sourceScale)], T)\n",
    "                outputs[(\"sample\", frameIdx, scale)] = pixelCoordinates\n",
    "                outputs[(\"color\", frameIdx, scale)] = F.grid_sample(inputs[(\"color\", frameIdx, sourceScale)],\n",
    "                                                                    outputs[((\"sample\", frameIdx, scale))],\n",
    "                                                                    padding_mode=\"border\")\n",
    "                outputs[(\"color_identity\", frameIdx, scale)] = inputs[(\"color\", frameIdx, sourceScale)]\n",
    "\n",
    "    def computeDepthErrors(self, depthGroundTruth, depthPred):\n",
    "        threshold = torch.max((depthGroundTruth/depthPred), (depthPred/depthGroundTruth))\n",
    "        a1 = (threshold < 1.25).float().mean()\n",
    "        a2 = (threshold < 1.25**2).float().mean()\n",
    "        a3 = (threshold < 1.25**3).float().mean()\n",
    "        rootMeanSquaredError = (depthGroundTruth - depthPred)**2\n",
    "        rootMeanSquaredError = torch.sqrt(rootMeanSquaredError.mean())\n",
    "        rootMeanSquaredErrorLog = (torch.log(depthGroundTruth) - torch.log(depthPred))**2\n",
    "        rootMeanSquaredErrorLog = torch.sqrt(rootMeanSquaredErrorLog.mean())\n",
    "        absolute = torch.mean(torch.abs(depthGroundTruth - depthPred)/depthGroundTruth)\n",
    "        squared = torch.mean(((depthGroundTruth - depthPred)**2)/depthGroundTruth)\n",
    "        return absolute, squared, rootMeanSquaredError, rootMeanSquaredErrorLog, a1, a2, a3\n",
    "\n",
    "    def computeDepthLosses(self, inputs, outputs, losses):\n",
    "        depthPred = outputs[(\"depth\", 0, 0)]\n",
    "        depthPred = torch.clamp(F.interpolate(depthPred, [375, 1242], mode='bilinear',\n",
    "                                              align_corners=False), 1e-3, 80)\n",
    "        depthPred = depthPred.detach()\n",
    "        depthGroundTruth = inputs[\"depth_gt\"]\n",
    "        mask = depthGroundTruth > 0\n",
    "        cropMask = torch.zeros_like(mask)\n",
    "        cropMask[:, :, 153:371, 44:1197] = 1\n",
    "        mask = mask * cropMask\n",
    "        depthGroundTruth = depthGroundTruth[mask]\n",
    "        depthPred = depthPred[mask]\n",
    "        depthPred *= torch.median(depthGroundTruth)/torch.median(depthPred)\n",
    "        depthPred = torch.clamp(depthPred, 1e-3, 80)\n",
    "        depthErrors = self.computeDepthErrors(depthGroundTruth, depthPred)\n",
    "        for i, name in enumerate(self.depthMetricNames):\n",
    "            losses[name] = np.array(depthErrors[i].cpu())\n",
    "\n",
    "    def computeReprojectionLoss(self, pred, target):\n",
    "        absDiff = torch.abs(target - pred)\n",
    "        l1Loss = absDiff.mean(1, True)\n",
    "        ssim_loss = self.ssim(pred, target).mean(1, True)\n",
    "        return 0.85*ssim_loss + 0.15*l1Loss\n",
    "\n",
    "    def getSmoothLoss(self, disp, img):\n",
    "        gradientDispX = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "        gradientDispY = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "        gradientImgX = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "        gradientImgY = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "        gradientDispX *= torch.exp(-gradientImgX)\n",
    "        gradientDispY *= torch.exp(-gradientImgY)\n",
    "        return gradientDispX.mean() + gradientDispY.mean()\n",
    "\n",
    "    def computeLosses(self, inputs, outputs):\n",
    "        losses = {}\n",
    "        totalLoss = 0\n",
    "        for scale in range(self.numScales):\n",
    "            loss = 0\n",
    "            reprojectionLoss = []\n",
    "            sourceScale = 0\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            color = inputs[(\"color\", 0, scale)]\n",
    "            target = inputs[(\"color\", 0, sourceScale)]\n",
    "            for frameIdx in self.frameIdxs[1:]:\n",
    "                pred = outputs[(\"color\", frameIdx, scale)]\n",
    "                reprojectionLoss.append(self.computeReprojectionLoss(pred, target))\n",
    "            reprojectionLoss = torch.cat(reprojectionLoss, 1)\n",
    "            identityReprojectionLoss = []\n",
    "            for frameIdx in self.frameIdxs[1:]:\n",
    "                pred = inputs[(\"color\", frameIdx, sourceScale)]\n",
    "                identityReprojectionLoss.append(self.computeReprojectionLoss(pred, target))\n",
    "            identityReprojectionLoss = torch.cat(identityReprojectionLoss, 1)\n",
    "            identityReprojectionLoss += torch.randn(identityReprojectionLoss.shape, device=self.device) * 0.00001\n",
    "            combined = torch.cat((identityReprojectionLoss, reprojectionLoss), 1)\n",
    "            if combined.shape[1] == 1:\n",
    "                toOptimise = combined\n",
    "            else:\n",
    "                toOptimise, idxs = torch.min(combined, dim=1)\n",
    "            outputs[\"identity_selection/{}\".format(scale)] = (idxs > identityReprojectionLoss.shape[1] - 1).float()\n",
    "            loss += toOptimise.mean()\n",
    "            meanDisp = disp.mean(2, True).mean(3, True)\n",
    "            normDisp = disp / (meanDisp + 1e-7)\n",
    "            smoothLoss = self.getSmoothLoss(normDisp, color)\n",
    "            loss += (1e-3 * smoothLoss)/(2**scale)\n",
    "            totalLoss += loss\n",
    "            losses[\"loss/{}\".format(scale)] = loss\n",
    "        totalLoss /= self.numScales\n",
    "        losses[\"loss\"] = totalLoss\n",
    "        return losses\n",
    "\n",
    "    def processBatch(self, inputs):\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(self.device)\n",
    "        origScaleColorAug = torch.cat([inputs[(\"color_aug\", fi, 0)] for fi in self.frameIdxs])\n",
    "        allFrameFeatures = self.models[\"encoder\"](origScaleColorAug)\n",
    "        allFrameFeatures = [torch.split(f, self.batchSize) for f in allFrameFeatures]\n",
    "        features = {}\n",
    "        for i, frameIdx in enumerate(self.frameIdxs):\n",
    "            features[frameIdx] = [f[i] for f in allFrameFeatures]\n",
    "        outputs = self.models[\"decoder\"](features[0])\n",
    "        outputs.update(self.predictPoses(inputs, features))\n",
    "        self.generateImagePredictions(inputs, outputs)\n",
    "        losses = self.computeLosses(inputs, outputs)\n",
    "        return outputs, losses\n",
    "\n",
    "    def runEpoch(self):\n",
    "        self.lrScheduler.step()\n",
    "        self.setTrain()\n",
    "        for batchIdx, inputs in enumerate(self.trainLoader):\n",
    "            startTime = time.time()\n",
    "            outputs, losses = self.processBatch(inputs)\n",
    "            self.optimizer.zero_grad()\n",
    "            losses[\"loss\"].backward()\n",
    "            self.optimizer.step()\n",
    "            duration = time.time() - startTime\n",
    "            early_phase = batchIdx % 200 == 0 and self.step < 2000\n",
    "            late_phase = self.step % 1000 == 0\n",
    "            if early_phase or late_phase:\n",
    "                self.logTime(batchIdx, duration, losses[\"loss\"].cpu().data)\n",
    "                self.computeDepthLosses(inputs, outputs, losses)\n",
    "                self.log(\"train\", inputs, outputs, losses)\n",
    "                self.val()\n",
    "            self.step += 1\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Total Trainable Parameters : {}\".format(self.totalTrainableParams))\n",
    "        print(\"Total Steps : {}\".format(self.numSteps))\n",
    "        self.epoch = 0\n",
    "        self.step = 0\n",
    "        self.startTime = time.time()\n",
    "        for self.epoch in range(self.epochs):\n",
    "            print(\"Training --- Epoch : {}\".format(self.epoch))\n",
    "            self.runEpoch()\n",
    "            self.saveModel()\n",
    "\n",
    "    def val(self):\n",
    "        self.setEval()\n",
    "        try:\n",
    "            inputs = self.valIterator.next()\n",
    "        except:\n",
    "            self.valIterator = iter(self.valLoader)\n",
    "            inputs = self.valIterator.next()\n",
    "        with torch.no_grad():\n",
    "            outputs, losses = self.processBatch(inputs)\n",
    "            self.computeDepthLosses(inputs, outputs, losses)\n",
    "            self.log(\"val\", inputs, outputs, losses)\n",
    "            del inputs, outputs, losses\n",
    "        self.setTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mp6021/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters : 36867202\n",
      "Total Steps : 33160\n",
      "Training --- Epoch : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mp6021/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/mp6021/.local/lib/python3.8/site-packages/torch/nn/functional.py:4193: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :   0 | Batch :       0, examples/s:  25.3, loss : 0.15857, time elapsed: 00h00m14s, time left: 00h00m00s\n",
      "Epoch :   0 | Batch :     200, examples/s:  28.3, loss : 0.13839, time elapsed: 00h04m41s, time left: 12h53m02s\n",
      "Epoch :   0 | Batch :     400, examples/s:  28.4, loss : 0.14211, time elapsed: 00h09m04s, time left: 12h23m23s\n",
      "Epoch :   0 | Batch :     600, examples/s:  28.4, loss : 0.13533, time elapsed: 00h13m37s, time left: 12h19m10s\n",
      "Epoch :   0 | Batch :     800, examples/s:  28.3, loss : 0.13588, time elapsed: 00h18m02s, time left: 12h09m29s\n",
      "Epoch :   0 | Batch :    1000, examples/s:  28.3, loss : 0.12581, time elapsed: 00h22m27s, time left: 12h02m22s\n",
      "Epoch :   0 | Batch :    1200, examples/s:  28.3, loss : 0.12823, time elapsed: 00h26m48s, time left: 11h53m52s\n",
      "Epoch :   0 | Batch :    1400, examples/s:  28.3, loss : 0.13625, time elapsed: 00h31m13s, time left: 11h48m21s\n",
      "Epoch :   0 | Batch :    1600, examples/s:  28.3, loss : 0.12785, time elapsed: 00h35m42s, time left: 11h44m28s\n",
      "Training --- Epoch : 1\n",
      "Epoch :   1 | Batch :       0, examples/s:  27.2, loss : 0.13342, time elapsed: 00h37m11s, time left: 11h46m42s\n",
      "Epoch :   1 | Batch :     200, examples/s:  28.4, loss : 0.12997, time elapsed: 00h41m39s, time left: 11h41m43s\n",
      "Epoch :   1 | Batch :     342, examples/s:  28.3, loss : 0.12288, time elapsed: 00h44m45s, time left: 11h37m16s\n",
      "Epoch :   1 | Batch :    1342, examples/s:  28.3, loss : 0.12907, time elapsed: 01h06m43s, time left: 11h10m43s\n",
      "Training --- Epoch : 2\n",
      "Epoch :   2 | Batch :     684, examples/s:  27.0, loss : 0.10826, time elapsed: 01h28m59s, time left: 10h48m42s\n",
      "Training --- Epoch : 3\n",
      "Epoch :   3 | Batch :      26, examples/s:  28.3, loss : 0.11330, time elapsed: 01h51m31s, time left: 10h28m05s\n",
      "Epoch :   3 | Batch :    1026, examples/s:  28.3, loss : 0.10771, time elapsed: 02h14m22s, time left: 10h08m14s\n",
      "Training --- Epoch : 4\n",
      "Epoch :   4 | Batch :     368, examples/s:  28.4, loss : 0.10967, time elapsed: 02h36m51s, time left: 09h46m13s\n",
      "Epoch :   4 | Batch :    1368, examples/s:  28.3, loss : 0.12101, time elapsed: 02h59m07s, time left: 09h23m21s\n",
      "Training --- Epoch : 5\n",
      "Epoch :   5 | Batch :     710, examples/s:  28.3, loss : 0.10586, time elapsed: 03h21m29s, time left: 09h00m52s\n",
      "Training --- Epoch : 6\n",
      "Epoch :   6 | Batch :      52, examples/s:  28.3, loss : 0.09830, time elapsed: 03h43m47s, time left: 08h38m18s\n",
      "Epoch :   6 | Batch :    1052, examples/s:  28.4, loss : 0.10466, time elapsed: 04h05m55s, time left: 08h15m25s\n",
      "Training --- Epoch : 7\n",
      "Epoch :   7 | Batch :     394, examples/s:  28.3, loss : 0.10752, time elapsed: 04h28m07s, time left: 07h52m48s\n",
      "Epoch :   7 | Batch :    1394, examples/s:  28.4, loss : 0.09247, time elapsed: 04h49m42s, time left: 07h29m16s\n",
      "Training --- Epoch : 8\n",
      "Epoch :   8 | Batch :     736, examples/s:  28.3, loss : 0.09719, time elapsed: 05h12m29s, time left: 07h07m40s\n",
      "Training --- Epoch : 9\n",
      "Epoch :   9 | Batch :      78, examples/s:  28.3, loss : 0.09155, time elapsed: 05h36m26s, time left: 06h47m18s\n",
      "Epoch :   9 | Batch :    1078, examples/s:  28.3, loss : 0.10357, time elapsed: 05h59m15s, time left: 06h25m18s\n",
      "Training --- Epoch : 10\n",
      "Epoch :  10 | Batch :     420, examples/s:  28.3, loss : 0.10807, time elapsed: 06h22m14s, time left: 06h03m21s\n",
      "Epoch :  10 | Batch :    1420, examples/s:  28.3, loss : 0.09483, time elapsed: 06h45m52s, time left: 05h41m49s\n",
      "Training --- Epoch : 11\n",
      "Epoch :  11 | Batch :     762, examples/s:  28.3, loss : 0.09692, time elapsed: 07h10m07s, time left: 05h20m33s\n"
     ]
    }
   ],
   "source": [
    "t.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
